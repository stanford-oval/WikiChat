# Wikipedia Search API

## Endpoint
`POST /search`

## Description
This endpoint allows you to search in text, table and infoboxes of 10 Wikipedias (ğŸ‡ºğŸ‡¸ English, ğŸ‡¨ğŸ‡³ Chinese, ğŸ‡ªğŸ‡¸ Spanish, ğŸ‡µğŸ‡¹ Portuguese, ğŸ‡·ğŸ‡º Russian, ğŸ‡©ğŸ‡ª German, ğŸ‡®ğŸ‡· Farsi, ğŸ‡¯ğŸ‡µ Japanese, ğŸ‡«ğŸ‡· French, ğŸ‡®ğŸ‡¹ Italian) with various query parameters.

It is currently retrieving from the Wikipedia dump of August 1, 2024.

The search endpoint is a hosted version of `retrieval/retriever_server.py`.
Specifically, it uses the state-of-the-art multilingual vector embedding models for high quality search results.
It also supports batch queries.

## Request Body
The request body should be a JSON object with the following fields:

- `query`: A string or a list of strings representing the search queries.
- `num_blocks`: An integer representing the number of items to retrieve.
- `languages`: (Optional) A string or a list of strings representing the language codes to filter the search results.

### Example
Search for the 3 most relevant text, table or infobox in the any of the 10 Wikipedia languages.
```http
POST https://search.genie.stanford.edu/wikipedia
Content-Type: application/json

{
  "query": ["What is GPT-4?", "What is LLaMA-3?"],
  "num_blocks": 3,
}
```

or equivalently, run

```
curl -X POST https://search.genie.stanford.edu/wikipedia -H "Content-Type: application/json" -d '{"query": ["What is GPT-4?", "What is LLaMA-3?"], "num_blocks": 3}'
```

Response:
```json
[
    {
        "score": [
            0.6902604699134827,
            0.6895850896835327,
            0.6812092661857605
        ],
        "text": [
            "GPT-4ï¼ˆã‚¸ãƒ¼ãƒ”ãƒ¼ãƒ†ã‚£ãƒ¼ãƒ•ã‚©ãƒ¼ã€Generative Pre-trained Transformer 4ï¼‰ã¨ã¯ã€OpenAI (in English: OpenAI)ã«ã‚ˆã£ã¦é–‹ç™ºã•ã‚ŒãŸãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ« (in English: Multimodal learning)ï¼ˆè‹±èªç‰ˆï¼‰å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ« (in English: Large language model)ã§ã‚ã‚‹ã€‚2023å¹´3æœˆ14æ—¥ã«å…¬é–‹ã•ã‚ŒãŸã€‚è‡ªç„¶è¨€èªå‡¦ç† (in English: Natural language processing)ã«Transformer (æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«) (in English: Transformer)ã‚’æ¡ç”¨ã—ã¦ãŠã‚Šã€æ•™å¸«ãªã—å­¦ç¿’ (in English: Unsupervised learning)ã«ã‚ˆã£ã¦å¤§è¦æ¨¡ãªãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ (in English: Neural network)ã‚’å­¦ç¿’ã•ã›ã€ãã®å¾Œã€äººé–“ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‹ã‚‰ã®å¼·åŒ–å­¦ç¿’ (in English: Reinforcement learning from human feedback)ï¼ˆRLHFï¼‰ã‚’è¡Œã£ã¦ã„ã‚‹ã€‚",
            "Generative Pre-trained Transformer 4 (GPT-4) Ã© um Modelo de linguagem grande (in English: Large language model) multimodal criado pela OpenAI (in English: OpenAI) e o quarto modelo da sÃ©rie GPT.  Foi lanÃ§ado em 14 de marÃ§o de 2023, e se tornou publicamente aberto de forma limitada por meio do ChatGPT (in English: ChatGPT) Plus, com o seu acesso Ã  API comercial sendo provido por uma lista de espera. Sendo um Transformador (in English: Transformer), foi prÃ©-treinado para prever o prÃ³ximo Token (informÃ¡tica) (usando dados pÃºblicos e \"licenciados de provedores terceirizados\"), e entÃ£o foi aperfeiÃ§oado atravÃ©s de uma tÃ©cnica de aprendizagem por reforÃ§o com humanos.",
            "GPT-4 es un modelo de Inteligencia artificial multimodal que puede generar texto a partir de diferentes tipos de entradas, como texto o imÃ¡genes. GPT-4 es un modelo multimodal porque puede Procesador de texto (in English: Word processor) y combinar diferentes modalidades de informaciÃ³n, como el Lengua natural (in English: Natural language) y la VisiÃ³n artificial (in English: Computer vision) Esto le da una ventaja sobre los modelos que solo pueden manejar una modalidad, ya que puede aprovechar el contexto y el conocimiento de mÃºltiples fuentes.GPT-4 utiliza una tÃ©cnica llamada fusiÃ³n cruzada, que le permite integrar InformaciÃ³n (in English: Information) de diferentes modalidades en una sola representaciÃ³n, lo que mejora su capacidad de Entendimiento (in English: Understanding) y generaciÃ³n"
        ],
        "title": [
            "GPT-4",
            "GPT-4",
            "Inteligencia artificial multimodal (in English: Multimodal artificial intelligence)"
        ],
        "full_section_title": [
            "GPT-4",
            "GPT-4",
            "Inteligencia artificial multimodal (in English: Multimodal artificial intelligence) > GPT-4"
        ],
        "block_type": [
            "text",
            "text",
            "text"
        ],
        "language": [
            "ja",
            "pt",
            "es"
        ],
        "last_edit_date": [
            "2024-03-01T14:03:10Z",
            "2024-01-10T18:38:38Z",
            "2024-03-28T22:44:02Z"
        ],
        "prob": [
            0.33441298757005206,
            0.3341872079017531,
            0.3313998045281948
        ]
    },
    {
        "score": [
            0.5762701034545898,
            0.5515922904014587,
            0.5366302728652954
        ],
        "text": [
            "LLaMA (Large Language Model Meta AI) Ã© um grande modelo de linguagem (LLM) lanÃ§ado pela Meta AI em fevereiro de 2023. Uma variedade de modelo foi treinada, variando de 7 bilhÃµes a 65 bilhÃµes. Os desenvolvedores do LLaMA relataram que o desempenho do modelo de 13 bilhÃµes de parÃ¢metros na maioria dos benchmarks NLP excedeu o do muito maior GPT-3 (in English: GPT-3) (com 175 bilhÃµes de parÃ¢metros) e que o maior modelo era competitivo com modelos de Ãºltima geraÃ§Ã£o, como PaLM e Chinchilla. Considerando que os LLMs mais poderosos geralmente sÃ£o acessÃ­veis apenas por meio de APIs limitadas (se Ã© que existem), a Meta lanÃ§ou os modelo do LLaMA para a comunidade de pesquisa sob uma licenÃ§a nÃ£o comercial. Uma semana apÃ³s o lanÃ§amento do LLaMA, seus pesos vazaram para o pÃºblico no 4chan via BitTorrent (in English: BitTorrent).",
            "LLaMAï¼ˆLarge Language Model Meta AIï¼‰ã¯ã€Meta (ä¼æ¥­) (in English: Meta Platforms) ãŒ2023å¹´2æœˆã«ç™ºè¡¨ã—ãŸå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ« (in English: Large language model)ã€‚70å„„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‹ã‚‰650å„„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¾ã§ã€ã•ã¾ã–ã¾ãªã‚µã‚¤ã‚ºã®ãƒ¢ãƒ‡ãƒ«ãŒå­¦ç¿’ã•ã‚ŒãŸã€‚LLaMA ã®é–‹ç™ºè€…ã¯ã€130å„„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¢ãƒ‡ãƒ«ãŒã»ã¨ã‚“ã©ã®è‡ªç„¶è¨€èªå‡¦ç† (in English: Natural language processing)ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã«ãŠã„ã¦GPT-3 (in English: GPT-3)ï¼ˆ1750å„„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼‰ã®æ€§èƒ½ã‚’ä¸Šå›ã‚‹ã“ã¨ã€æœ€å¤§ã®ãƒ¢ãƒ‡ãƒ«ã¯ PaLM (in English: PaLM) ã‚„ Chinchilla ãªã©ã®æœ€å…ˆç«¯ãƒ¢ãƒ‡ãƒ«ã«åŒ¹æ•µã™ã‚‹ã“ã¨ã‚’å ±å‘Šã—ã¦ã„ã‚‹ã€‚å¾“æ¥ã€ã»ã¨ã‚“ã©ã®å¼·åŠ›ãªå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã¯é™ã‚‰ã‚ŒãŸ ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã‚¤ãƒ³ã‚¿ãƒ•ã‚§ãƒ¼ã‚¹ (in English: API) ã‚’é€šã˜ã¦ã—ã‹ã‚¢ã‚¯ã‚»ã‚¹ã§ããªã‹ã£ãŸãŒã€Meta ã¯ LLaMA ã®ãƒ¢ãƒ‡ãƒ«ã®ã‚¦ã‚§ã‚¤ãƒˆã‚’éå•†ç”¨ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã§ç ”ç©¶ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã«å…¬é–‹ã—ãŸã€‚LLaMAã®ãƒªãƒªãƒ¼ã‚¹ã‹ã‚‰1é€±é–“ã§ã€ãã®ã‚¦ã‚§ã‚¤ãƒˆãŒãƒªãƒ¼ã‚¯ã•ã‚ŒãŸã€‚",
            "Laminin subunit beta-3 is a protein that in humans is encoded by the LAMB3 gene. LAMB3 encodes the beta 3 subunit of laminin. Laminin is composed of three subunits (alpha, beta, and gamma), and refers to a family of basement membrane proteins. For example, LAMB3 serves as the beta chain in laminin-5. Mutations in LAMB3 have been identified as the cause of various types of epidermolysis bullosa. Two alternatively spliced transcript variants encoding the same protein have been found for this gene."
        ],
        "title": [
            "LLaMA",
            "LLaMA",
            "Laminin, beta 3"
        ],
        "full_section_title": [
            "LLaMA",
            "LLaMA",
            "Laminin, beta 3"
        ],
        "block_type": [
            "text",
            "text",
            "text"
        ],
        "language": [
            "pt",
            "ja",
            "en"
        ],
        "last_edit_date": [
            "2023-09-09T14:18:11Z",
            "2024-01-13T22:47:19Z",
            "2023-01-29T23:05:16Z"
        ],
        "prob": [
            0.34051134155877916,
            0.33221110342082927,
            0.32727755502039146
        ]
    }
]
```